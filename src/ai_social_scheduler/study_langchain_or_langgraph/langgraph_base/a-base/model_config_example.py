"""
æ¨¡å‹é…ç½®ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨ LangGraph ä¸­é…ç½®ä»£ç†ä½¿ç”¨çš„èŠå¤©æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š

1. æŒ‰åç§°æŒ‡å®šæ¨¡å‹ - ä½¿ç”¨æ¨¡å‹åç§°å­—ç¬¦ä¸²é…ç½®ä»£ç†
2. ä½¿ç”¨ init_chat_model - ç®€åŒ–æ¨¡å‹åˆå§‹åŒ–å¹¶æä¾›å¯é…ç½®å‚æ•°
3. ä½¿ç”¨ç‰¹å®šæä¾›å•†çš„ LLM - ç›´æ¥å®ä¾‹åŒ–ç‰¹å®šæä¾›å•†çš„æ¨¡å‹ç±»
4. ç¦ç”¨æµå¼ä¼ è¾“ - åœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶ç¦ç”¨å•ä¸ª LLM ä»¤ç‰Œçš„æµå¼ä¼ è¾“
5. æ·»åŠ æ¨¡å‹å›é€€ - ä¸ºä¸åŒçš„æ¨¡å‹æˆ–ä¸åŒçš„ LLM æä¾›å•†æ·»åŠ å›é€€

æ‰§è¡Œå‘½ä»¤ï¼š
uv run python -m ai_social_scheduler.study_langchain_or_langgraph.langgraph_base.a-base.model_config_example

"""

import os

from langchain_core.messages import HumanMessage
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent

from ai_social_scheduler.client import QwenClient


# ==================== å·¥å…·å®šä¹‰ ====================

@tool
def calculator(expression: str) -> str:
    """
    è®¡ç®—æ•°å­¦è¡¨è¾¾å¼
    
    Args:
        expression: æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚ "2 + 2" æˆ– "10 * 5"
    
    Returns:
        str: è®¡ç®—ç»“æœ
    """
    try:
        # ç®€å•çš„å®‰å…¨è®¡ç®—ï¼ˆä»…æ”¯æŒåŸºæœ¬è¿ç®—ï¼‰
        result = eval(expression.replace(" ", ""))
        return f"è®¡ç®—ç»“æœ: {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {str(e)}"


# ==================== 1. æŒ‰åç§°æŒ‡å®šæ¨¡å‹ ====================

def example_model_by_name():
    """
    ç¤ºä¾‹ 1: æŒ‰åç§°æŒ‡å®šæ¨¡å‹
    
    ä½¿ç”¨æ¨¡å‹åç§°å­—ç¬¦ä¸²é…ç½®ä»£ç†ã€‚è¿™æ˜¯æœ€ç®€å•çš„æ–¹å¼ã€‚
    
    æ³¨æ„ï¼šæ­¤æ–¹æ³•éœ€è¦æ¨¡å‹æä¾›å•†æ”¯æŒé€šè¿‡åç§°å­—ç¬¦ä¸²åˆå§‹åŒ–ã€‚
    å¯¹äº OpenAIã€Anthropic ç­‰ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚
    å¯¹äºè‡ªå®šä¹‰å®¢æˆ·ç«¯ï¼ˆå¦‚ QwenClientï¼‰ï¼Œéœ€è¦å…ˆåˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹ã€‚
    """
    print("=" * 80)
    print("ç¤ºä¾‹ 1: æŒ‰åç§°æŒ‡å®šæ¨¡å‹")
    print("=" * 80)
    print()
    
    # æ–¹å¼ 1: ä½¿ç”¨ QwenClientï¼ˆé¡¹ç›®è‡ªå®šä¹‰å®¢æˆ·ç«¯ï¼‰
    print("æ–¹å¼ 1: ä½¿ç”¨ QwenClient")
    print("-" * 80)
    
    qwen_client = QwenClient(
        model="qwen-plus",
        temperature=0.7,
    )
    
    agent = create_react_agent(
        model=qwen_client.client,
        tools=[calculator],
    )
    
    response = agent.invoke({
        "messages": [HumanMessage(content="è®¡ç®— 15 * 8 çš„ç»“æœ")]
    })
    
    if "messages" in response and response["messages"]:
        last_message = response["messages"][-1]
        print(f"æ¨¡å‹: {qwen_client.model}")
        print(f"é—®é¢˜: è®¡ç®— 15 * 8 çš„ç»“æœ")
        print(f"å›ç­”: {last_message.content}")
    
    print()
    
    # æ–¹å¼ 2: ä½¿ç”¨ OpenAIï¼ˆå¦‚æœé…ç½®äº† OPENAI_API_KEYï¼‰
    print("æ–¹å¼ 2: ä½¿ç”¨ OpenAIï¼ˆéœ€è¦é…ç½® OPENAI_API_KEYï¼‰")
    print("-" * 80)
    
    if os.getenv("OPENAI_API_KEY"):
        try:
            from langchain_openai import ChatOpenAI
            
            openai_model = ChatOpenAI(
                model="gpt-4",
                temperature=0,
            )
            
            agent = create_react_agent(
                model=openai_model,
                tools=[calculator],
            )
            
            response = agent.invoke({
                "messages": [HumanMessage(content="è®¡ç®— 20 + 30 çš„ç»“æœ")]
            })
            
            if "messages" in response and response["messages"]:
                last_message = response["messages"][-1]
                print(f"æ¨¡å‹: gpt-4")
                print(f"é—®é¢˜: è®¡ç®— 20 + 30 çš„ç»“æœ")
                print(f"å›ç­”: {last_message.content}")
        except Exception as e:
            print(f"âš ï¸  OpenAI é…ç½®é”™è¯¯: {e}")
            print("æç¤º: è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    else:
        print("âš ï¸  æœªé…ç½® OPENAI_API_KEYï¼Œè·³è¿‡ OpenAI ç¤ºä¾‹")
    
    print()
    print("âœ… æŒ‰åç§°æŒ‡å®šæ¨¡å‹ç¤ºä¾‹å®Œæˆ")
    print()


# ==================== 2. ä½¿ç”¨ init_chat_model ====================

def example_init_chat_model():
    """
    ç¤ºä¾‹ 2: ä½¿ç”¨ init_chat_model
    
    init_chat_model å·¥å…·ç®€åŒ–äº†æ¨¡å‹åˆå§‹åŒ–ï¼Œå¹¶æä¾›äº†å¯é…ç½®å‚æ•°ã€‚
    æ”¯æŒå¤šç§æ¨¡å‹æä¾›å•†ï¼šOpenAIã€Anthropicã€Azureã€Google Geminiã€AWS Bedrock ç­‰ã€‚
    
    æ³¨æ„ï¼šéœ€è¦å®‰è£…ç›¸åº”çš„ä¾èµ–åŒ…ï¼Œå¦‚ï¼š
    - pip install -U "langchain[openai]"
    - pip install -U "langchain[anthropic]"
    """
    print("=" * 80)
    print("ç¤ºä¾‹ 2: ä½¿ç”¨ init_chat_model")
    print("=" * 80)
    print()
    
    # å°è¯•ä½¿ç”¨ init_chat_modelï¼ˆå¦‚æœå¯ç”¨ï¼‰
    try:
        from langchain.chat_models import init_chat_model
        
        print("ä½¿ç”¨ init_chat_model åˆå§‹åŒ–æ¨¡å‹...")
        print("-" * 80)
        
        # ç¤ºä¾‹ï¼šä½¿ç”¨ OpenAIï¼ˆå¦‚æœé…ç½®äº† API Keyï¼‰
        if os.getenv("OPENAI_API_KEY"):
            try:
                model = init_chat_model(
                    "openai:gpt-4",
                    temperature=0,
                    max_tokens=1000,
                )
                
                agent = create_react_agent(
                    model=model,
                    tools=[calculator],
                )
                
                response = agent.invoke({
                    "messages": [HumanMessage(content="è®¡ç®— 100 / 4 çš„ç»“æœ")]
                })
                
                if "messages" in response and response["messages"]:
                    last_message = response["messages"][-1]
                    print(f"æ¨¡å‹: openai:gpt-4")
                    print(f"é—®é¢˜: è®¡ç®— 100 / 4 çš„ç»“æœ")
                    print(f"å›ç­”: {last_message.content}")
            except Exception as e:
                print(f"âš ï¸  OpenAI åˆå§‹åŒ–é”™è¯¯: {e}")
        
        # ç¤ºä¾‹ï¼šä½¿ç”¨ Anthropicï¼ˆå¦‚æœé…ç½®äº† API Keyï¼‰
        elif os.getenv("ANTHROPIC_API_KEY"):
            try:
                model = init_chat_model(
                    "anthropic:claude-3-5-sonnet-latest",
                    temperature=0,
                )
                
                agent = create_react_agent(
                    model=model,
                    tools=[calculator],
                )
                
                response = agent.invoke({
                    "messages": [HumanMessage(content="è®¡ç®— 50 - 25 çš„ç»“æœ")]
                })
                
                if "messages" in response and response["messages"]:
                    last_message = response["messages"][-1]
                    print(f"æ¨¡å‹: anthropic:claude-3-5-sonnet-latest")
                    print(f"é—®é¢˜: è®¡ç®— 50 - 25 çš„ç»“æœ")
                    print(f"å›ç­”: {last_message.content}")
            except Exception as e:
                print(f"âš ï¸  Anthropic åˆå§‹åŒ–é”™è¯¯: {e}")
        else:
            print("âš ï¸  æœªé…ç½® OPENAI_API_KEY æˆ– ANTHROPIC_API_KEY")
            print("æç¤º: è¯·è®¾ç½®ç›¸åº”çš„ API Key ç¯å¢ƒå˜é‡")
            print("æˆ–è€…ä½¿ç”¨ QwenClientï¼ˆé¡¹ç›®é»˜è®¤å®¢æˆ·ç«¯ï¼‰")
            
            # ä½¿ç”¨ QwenClient ä½œä¸ºæ›¿ä»£ç¤ºä¾‹
            qwen_client = QwenClient(
                model="qwen-plus",
                temperature=0,
            )
            
            agent = create_react_agent(
                model=qwen_client.client,
                tools=[calculator],
            )
            
            response = agent.invoke({
                "messages": [HumanMessage(content="è®¡ç®— 12 * 6 çš„ç»“æœ")]
            })
            
            if "messages" in response and response["messages"]:
                last_message = response["messages"][-1]
                print(f"æ¨¡å‹: {qwen_client.model} (ä½¿ç”¨ QwenClient)")
                print(f"é—®é¢˜: è®¡ç®— 12 * 6 çš„ç»“æœ")
                print(f"å›ç­”: {last_message.content}")
        
    except ImportError:
        print("âš ï¸  init_chat_model ä¸å¯ç”¨")
        print("æç¤º: è¯·å®‰è£… langchain åŒ…: pip install -U langchain")
        print("æˆ–è€…ä½¿ç”¨ç‰¹å®šæä¾›å•†çš„æ¨¡å‹ç±»ï¼ˆè§ç¤ºä¾‹ 3ï¼‰")
    
    print()
    print("âœ… ä½¿ç”¨ init_chat_model ç¤ºä¾‹å®Œæˆ")
    print()


# ==================== 3. ä½¿ç”¨ç‰¹å®šæä¾›å•†çš„ LLM ====================

def example_provider_specific_llm():
    """
    ç¤ºä¾‹ 3: ä½¿ç”¨ç‰¹å®šæä¾›å•†çš„ LLM
    
    å¦‚æœæ¨¡å‹æä¾›å•†æ— æ³•é€šè¿‡ init_chat_model è·å¾—ï¼Œæ‚¨å¯ä»¥ç›´æ¥å®ä¾‹åŒ–è¯¥æä¾›å•†çš„æ¨¡å‹ç±»ã€‚
    è¯¥æ¨¡å‹å¿…é¡»å®ç° BaseChatModel æ¥å£å¹¶æ”¯æŒå·¥å…·è°ƒç”¨ã€‚
    
    æœ¬ç¤ºä¾‹å±•ç¤ºï¼š
    - ä½¿ç”¨ QwenClientï¼ˆé¡¹ç›®è‡ªå®šä¹‰å®¢æˆ·ç«¯ï¼‰
    - ä½¿ç”¨ OpenAI ChatOpenAI
    - ä½¿ç”¨ Anthropic ChatAnthropic
    """
    print("=" * 80)
    print("ç¤ºä¾‹ 3: ä½¿ç”¨ç‰¹å®šæä¾›å•†çš„ LLM")
    print("=" * 80)
    print()
    
    # æ–¹å¼ 1: ä½¿ç”¨ QwenClientï¼ˆé¡¹ç›®è‡ªå®šä¹‰å®¢æˆ·ç«¯ï¼‰
    print("æ–¹å¼ 1: ä½¿ç”¨ QwenClient")
    print("-" * 80)
    
    qwen_client = QwenClient(
        model="qwen-plus",
        temperature=0.7,
        max_tokens=2048,
    )
    
    agent = create_react_agent(
        model=qwen_client.client,
        tools=[calculator],
    )
    
    response = agent.invoke({
        "messages": [HumanMessage(content="è®¡ç®— 8 * 9 çš„ç»“æœ")]
    })
    
    if "messages" in response and response["messages"]:
        last_message = response["messages"][-1]
        print(f"æ¨¡å‹: {qwen_client.model}")
        print(f"æ¸©åº¦: {qwen_client.temperature}")
        print(f"æœ€å¤§ tokens: {qwen_client.max_tokens}")
        print(f"é—®é¢˜: è®¡ç®— 8 * 9 çš„ç»“æœ")
        print(f"å›ç­”: {last_message.content}")
    
    print()
    
    # æ–¹å¼ 2: ä½¿ç”¨ OpenAI ChatOpenAI
    print("æ–¹å¼ 2: ä½¿ç”¨ OpenAI ChatOpenAI")
    print("-" * 80)
    
    if os.getenv("OPENAI_API_KEY"):
        try:
            from langchain_openai import ChatOpenAI
            
            model = ChatOpenAI(
                model="gpt-4",
                temperature=0,
                max_tokens=2048,
            )
            
            agent = create_react_agent(
                model=model,
                tools=[calculator],
            )
            
            response = agent.invoke({
                "messages": [HumanMessage(content="è®¡ç®— 7 * 7 çš„ç»“æœ")]
            })
            
            if "messages" in response and response["messages"]:
                last_message = response["messages"][-1]
                print(f"æ¨¡å‹: gpt-4")
                print(f"æ¸©åº¦: 0")
                print(f"æœ€å¤§ tokens: 2048")
                print(f"é—®é¢˜: è®¡ç®— 7 * 7 çš„ç»“æœ")
                print(f"å›ç­”: {last_message.content}")
        except Exception as e:
            print(f"âš ï¸  OpenAI é”™è¯¯: {e}")
    else:
        print("âš ï¸  æœªé…ç½® OPENAI_API_KEYï¼Œè·³è¿‡ OpenAI ç¤ºä¾‹")
    
    print()
    
    # æ–¹å¼ 3: ä½¿ç”¨ Anthropic ChatAnthropic
    print("æ–¹å¼ 3: ä½¿ç”¨ Anthropic ChatAnthropic")
    print("-" * 80)
    
    if os.getenv("ANTHROPIC_API_KEY"):
        try:
            from langchain_anthropic import ChatAnthropic
            
            model = ChatAnthropic(
                model="claude-3-5-sonnet-latest",
                temperature=0,
                max_tokens=2048,
            )
            
            agent = create_react_agent(
                model=model,
                tools=[calculator],
            )
            
            response = agent.invoke({
                "messages": [HumanMessage(content="è®¡ç®— 6 * 6 çš„ç»“æœ")]
            })
            
            if "messages" in response and response["messages"]:
                last_message = response["messages"][-1]
                print(f"æ¨¡å‹: claude-3-5-sonnet-latest")
                print(f"æ¸©åº¦: 0")
                print(f"æœ€å¤§ tokens: 2048")
                print(f"é—®é¢˜: è®¡ç®— 6 * 6 çš„ç»“æœ")
                print(f"å›ç­”: {last_message.content}")
        except Exception as e:
            print(f"âš ï¸  Anthropic é”™è¯¯: {e}")
    else:
        print("âš ï¸  æœªé…ç½® ANTHROPIC_API_KEYï¼Œè·³è¿‡ Anthropic ç¤ºä¾‹")
    
    print()
    print("âœ… ä½¿ç”¨ç‰¹å®šæä¾›å•†çš„ LLM ç¤ºä¾‹å®Œæˆ")
    print()


# ==================== 4. ç¦ç”¨æµå¼ä¼ è¾“ ====================

def example_disable_streaming():
    """
    ç¤ºä¾‹ 4: ç¦ç”¨æµå¼ä¼ è¾“
    
    è¦ç¦ç”¨å•ä¸ª LLM ä»¤ç‰Œçš„æµå¼ä¼ è¾“ï¼Œå¯ä»¥åœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶è®¾ç½® disable_streaming=Trueã€‚
    è¿™åœ¨å¤šä»£ç†ç³»ç»Ÿä¸­å¾ˆæœ‰ç”¨ï¼Œç”¨äºæ§åˆ¶å“ªäº›ä»£ç†æµå¼ä¼ è¾“å…¶è¾“å‡ºã€‚
    
    æ³¨æ„ï¼šå…·ä½“çš„ç¦ç”¨æ–¹æ³•å–å†³äºä½¿ç”¨çš„ LLM å®¢æˆ·ç«¯å®ç°ã€‚
    """
    print("=" * 80)
    print("ç¤ºä¾‹ 4: ç¦ç”¨æµå¼ä¼ è¾“")
    print("=" * 80)
    print()
    
    # æ–¹å¼ 1: ä½¿ç”¨ init_chat_model ç¦ç”¨æµå¼ä¼ è¾“
    print("æ–¹å¼ 1: ä½¿ç”¨ init_chat_model ç¦ç”¨æµå¼ä¼ è¾“")
    print("-" * 80)
    
    try:
        from langchain.chat_models import init_chat_model
        
        if os.getenv("OPENAI_API_KEY"):
            try:
                model = init_chat_model(
                    "openai:gpt-4",
                    temperature=0,
                    disable_streaming=True,  # ç¦ç”¨æµå¼ä¼ è¾“
                )
                
                agent = create_react_agent(
                    model=model,
                    tools=[calculator],
                )
                
                print("ä½¿ç”¨éæµå¼æ¨¡å¼è°ƒç”¨ä»£ç†...")
                response = agent.invoke({
                    "messages": [HumanMessage(content="è®¡ç®— 5 * 5 çš„ç»“æœ")]
                })
                
                if "messages" in response and response["messages"]:
                    last_message = response["messages"][-1]
                    print(f"æ¨¡å‹: openai:gpt-4 (disable_streaming=True)")
                    print(f"å›ç­”: {last_message.content}")
            except Exception as e:
                print(f"âš ï¸  é”™è¯¯: {e}")
        else:
            print("âš ï¸  æœªé…ç½® OPENAI_API_KEY")
    except ImportError:
        print("âš ï¸  init_chat_model ä¸å¯ç”¨")
    
    print()
    
    # æ–¹å¼ 2: ä½¿ç”¨ QwenClientï¼ˆé€šè¿‡ invoke è€Œä¸æ˜¯ streamï¼‰
    print("æ–¹å¼ 2: ä½¿ç”¨ QwenClientï¼ˆéæµå¼è°ƒç”¨ï¼‰")
    print("-" * 80)
    
    qwen_client = QwenClient(
        model="qwen-plus",
        temperature=0.7,
    )
    
    agent = create_react_agent(
        model=qwen_client.client,
        tools=[calculator],
    )
    
    print("ä½¿ç”¨ invoke æ–¹æ³•ï¼ˆéæµå¼ï¼‰...")
    response = agent.invoke({
        "messages": [HumanMessage(content="è®¡ç®— 4 * 4 çš„ç»“æœ")]
    })
    
    if "messages" in response and response["messages"]:
        last_message = response["messages"][-1]
        print(f"æ¨¡å‹: {qwen_client.model}")
        print(f"è°ƒç”¨æ–¹å¼: invoke (éæµå¼)")
        print(f"å›ç­”: {last_message.content}")
    
    print()
    print("ğŸ’¡ æç¤º: åœ¨å¤šä»£ç†ç³»ç»Ÿä¸­ï¼Œæ‚¨å¯ä»¥ä¸ºä¸åŒçš„ä»£ç†é…ç½®ä¸åŒçš„æµå¼ä¼ è¾“è¡Œä¸º")
    print("âœ… ç¦ç”¨æµå¼ä¼ è¾“ç¤ºä¾‹å®Œæˆ")
    print()


# ==================== 5. æ·»åŠ æ¨¡å‹å›é€€ ====================

def example_model_fallbacks():
    """
    ç¤ºä¾‹ 5: æ·»åŠ æ¨¡å‹å›é€€
    
    æ‚¨å¯ä»¥ä½¿ç”¨ model.with_fallbacks([...]) ä¸ºä¸åŒçš„æ¨¡å‹æˆ–ä¸åŒçš„ LLM æä¾›å•†æ·»åŠ å›é€€ã€‚
    å½“ä¸»æ¨¡å‹å¤±è´¥æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å°è¯•ä½¿ç”¨å›é€€æ¨¡å‹ã€‚
    
    è¿™å¯¹äºæé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œå¯ç”¨æ€§éå¸¸æœ‰ç”¨ã€‚
    """
    print("=" * 80)
    print("ç¤ºä¾‹ 5: æ·»åŠ æ¨¡å‹å›é€€")
    print("=" * 80)
    print()
    
    # æ–¹å¼ 1: ä½¿ç”¨ init_chat_model æ·»åŠ å›é€€
    print("æ–¹å¼ 1: ä½¿ç”¨ init_chat_model æ·»åŠ å›é€€")
    print("-" * 80)
    
    try:
        from langchain.chat_models import init_chat_model
        
        # åˆ›å»ºå¸¦å›é€€çš„æ¨¡å‹
        # ä¸»æ¨¡å‹ï¼šAnthropic Claude
        # å›é€€æ¨¡å‹ï¼šOpenAI GPT-4
        if os.getenv("ANTHROPIC_API_KEY") and os.getenv("OPENAI_API_KEY"):
            try:
                model_with_fallbacks = (
                    init_chat_model("anthropic:claude-3-5-sonnet-latest")
                    .with_fallbacks([
                        init_chat_model("openai:gpt-4"),
                    ])
                )
                
                agent = create_react_agent(
                    model=model_with_fallbacks,
                    tools=[calculator],
                )
                
                print("ä½¿ç”¨å¸¦å›é€€çš„æ¨¡å‹è°ƒç”¨ä»£ç†...")
                print("ä¸»æ¨¡å‹: anthropic:claude-3-5-sonnet-latest")
                print("å›é€€æ¨¡å‹: openai:gpt-4")
                print("-" * 80)
                
                response = agent.invoke({
                    "messages": [HumanMessage(content="è®¡ç®— 3 * 3 çš„ç»“æœ")]
                })
                
                if "messages" in response and response["messages"]:
                    last_message = response["messages"][-1]
                    print(f"å›ç­”: {last_message.content}")
            except Exception as e:
                print(f"âš ï¸  é”™è¯¯: {e}")
        else:
            print("âš ï¸  éœ€è¦é…ç½® ANTHROPIC_API_KEY å’Œ OPENAI_API_KEY")
            print("æç¤º: ä½¿ç”¨ QwenClient ä½œä¸ºæ›¿ä»£ç¤ºä¾‹")
            
            # ä½¿ç”¨ QwenClient ä½œä¸ºæ›¿ä»£ç¤ºä¾‹
            qwen_client = QwenClient(
                model="qwen-plus",
                temperature=0.7,
            )
            
            agent = create_react_agent(
                model=qwen_client.client,
                tools=[calculator],
            )
            
            response = agent.invoke({
                "messages": [HumanMessage(content="è®¡ç®— 2 * 2 çš„ç»“æœ")]
            })
            
            if "messages" in response and response["messages"]:
                last_message = response["messages"][-1]
                print(f"æ¨¡å‹: {qwen_client.model} (å•æ¨¡å‹ï¼Œæ— å›é€€)")
                print(f"å›ç­”: {last_message.content}")
    
    except ImportError:
        print("âš ï¸  init_chat_model ä¸å¯ç”¨")
        print("æç¤º: è¯·å®‰è£… langchain åŒ…")
    
    print()
    
    # æ–¹å¼ 2: ä½¿ç”¨å¤šä¸ª QwenClient æ¨¡å‹ä½œä¸ºå›é€€
    print("æ–¹å¼ 2: ä½¿ç”¨å¤šä¸ª QwenClient æ¨¡å‹ä½œä¸ºå›é€€")
    print("-" * 80)
    
    try:
        from langchain.chat_models import init_chat_model
        
        # æ³¨æ„ï¼šQwenClient å¯èƒ½ä¸æ”¯æŒ with_fallbacks
        # è¿™é‡Œå±•ç¤ºæ¦‚å¿µï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®å…·ä½“å®ç°è°ƒæ•´
        qwen_client_primary = QwenClient(
            model="qwen-plus",
            temperature=0.7,
        )
        
        qwen_client_fallback = QwenClient(
            model="qwen-turbo",  # ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹ä½œä¸ºå›é€€
            temperature=0.7,
        )
        
        # å¦‚æœæ”¯æŒ with_fallbacksï¼Œå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š
        # model_with_fallbacks = qwen_client_primary.client.with_fallbacks([
        #     qwen_client_fallback.client
        # ])
        
        # è¿™é‡Œç›´æ¥ä½¿ç”¨ä¸»æ¨¡å‹
        agent = create_react_agent(
            model=qwen_client_primary.client,
            tools=[calculator],
        )
        
        print("ä¸»æ¨¡å‹: qwen-plus")
        print("å›é€€æ¨¡å‹: qwen-turbo (æ¦‚å¿µç¤ºä¾‹)")
        print("-" * 80)
        
        response = agent.invoke({
            "messages": [HumanMessage(content="è®¡ç®— 1 * 1 çš„ç»“æœ")]
        })
        
        if "messages" in response and response["messages"]:
            last_message = response["messages"][-1]
            print(f"å›ç­”: {last_message.content}")
    
    except Exception as e:
        print(f"âš ï¸  é”™è¯¯: {e}")
    
    print()
    print("ğŸ’¡ æç¤º: æ¨¡å‹å›é€€å¯ä»¥æé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œå¯ç”¨æ€§")
    print("âœ… æ·»åŠ æ¨¡å‹å›é€€ç¤ºä¾‹å®Œæˆ")
    print()


# ==================== ä¸»å‡½æ•° ====================

def main():
    """è¿è¡Œæ‰€æœ‰æ¨¡å‹é…ç½®ç¤ºä¾‹"""
    try:
        # ç¤ºä¾‹ 1: æŒ‰åç§°æŒ‡å®šæ¨¡å‹
        example_model_by_name()
        print("\n" + "=" * 80 + "\n")
        
        # ç¤ºä¾‹ 2: ä½¿ç”¨ init_chat_model
        example_init_chat_model()
        print("\n" + "=" * 80 + "\n")
        
        # ç¤ºä¾‹ 3: ä½¿ç”¨ç‰¹å®šæä¾›å•†çš„ LLM
        example_provider_specific_llm()
        print("\n" + "=" * 80 + "\n")
        
        # ç¤ºä¾‹ 4: ç¦ç”¨æµå¼ä¼ è¾“
        example_disable_streaming()
        print("\n" + "=" * 80 + "\n")
        
        # ç¤ºä¾‹ 5: æ·»åŠ æ¨¡å‹å›é€€
        example_model_fallbacks()
        
    except ValueError as e:
        print(f"\nâŒ é…ç½®é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿åœ¨ .env æ–‡ä»¶ä¸­é…ç½®äº†ç›¸åº”çš„ API Key")
        import traceback
        traceback.print_exc()
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

